{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b7c805-32ba-429c-a3fc-c415a05033c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Programming for Biomedical Informatics\n",
    "#### Week 7 - Network Construction Techniques\n",
    "\n",
    "Constructing networks that are useful representations of the underlying biological data is a complex task. In this notebook we will explore some key concepts that are used to incoporate data into networks and then refine those using a selection of methodologies.\n",
    "Quantifying the impact of the assumptions and decisions made in the network construction and refinement process is a key part of the experimental analysis of networks. This is often confounded by the lack of ground-truth data upon which to make decisions.\n",
    "\n",
    "Thanks to Sebestyen Kamp who developed parts of these scripts for a workshop on networks presented at ISMB2024 in Montreal, Canada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039d4ac",
   "metadata": {},
   "source": [
    "Files used in this analysis\n",
    "\n",
    "- ISMB_TCGA_GE.pkl - contains gene expression data for TCGA samples\n",
    "- correlation_matrices.pkl - contains correlation matrices for TCGA sample gene expression data\n",
    "- correlation_matrices_figure.png - figure showing the correlation heatmaps\n",
    "- gene_coexpression_network_pearson.gml - base network for the gene coexpression network\n",
    "- full_gene_coexpression_network.png - figure showing the full gene coexpression network\n",
    "\n",
    "These files can be downloaded from [here](https://datasync.ed.ac.uk/index.php/s/0DDNSGC4YHv0NMi) with password: 'pbi2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1907862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Biomedical Networks'''\n",
    "# standard libraries\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# scientific and data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "# astropy is a library for astronomy and astrophysics! but has some very nice statistical tools\n",
    "import astropy\n",
    "from astropy.stats import median_absolute_deviation\n",
    "# mygene is a library for querying gene information (though you could use eUtils etc.)\n",
    "import mygene\n",
    "\n",
    "# graph and network libraries\n",
    "import networkx as nx\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# some deprecation warnings that are safe to ignore can be silenced using the warnings library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96698571",
   "metadata": {},
   "source": [
    "We're going to be looking at some gene expression data from the cancer genome atlas.\n",
    "\n",
    "Note that it has multi-modal data - we will look at this in later lectures. We're going to concentrate on gene expression in this notebook\n",
    "\n",
    "- **Title**: The Cancer Genome Atlas Lung Adenocarcinoma (TCGA-LUAD)\n",
    "- **Main Focus**: Study of lung adenocarcinoma (a common type of lung cancer)\n",
    "- **Data Collected**: Genomic, epigenomic, transcriptomic, and proteomic data from lung adenocarcinoma samples\n",
    "- **Disease Types**:\n",
    "  - Acinar Cell Neoplasms\n",
    "  - Adenomas and Adenocarcinomas\n",
    "  - Cystic, Mucinous, and Serous Neoplasms\n",
    "- **Number of Cases**: 585 (498 with transcriptomic data)\n",
    "- **Data Accessibility**: Available on the NIH-GDC Data Portal\n",
    "\n",
    "- **Link**: [TCGA-LUAD Project Page](https://portal.gdc.cancer.gov/projects/TCGA-LUAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3204f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use these data locations for the session but you should download the files\n",
    "# using the link above and change the paths below to the correct location on your machine\n",
    "raw_data_dir = './data/data/raw'\n",
    "intermediate_data_dir = './data/data/intermediate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce5339-5c18-48c0-b95b-d21196029943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the gene expression data from a pickle file\n",
    "'''a pickle file is a serialized python object that can be saved to disk and loaded back into memory\n",
    "these can be very useful for sharing python objects. In this case we have a dictionary with the gene \n",
    "expression data'''\n",
    "\n",
    "with open(os.path.join(raw_data_dir,\"ISMB_TCGA_GE.pkl\"), 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# print the keys of the dictionary\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ac5a0",
   "metadata": {},
   "source": [
    "In order to construct a biological network, we are going to first:\n",
    "- examine the TCGA metadata \n",
    "- come up with useful strategies to tackle the large data size \n",
    "- create the basis of a biological network\n",
    "\n",
    "We're going to first familiarise ourselves with the data by looking at the meta-data that \\\n",
    "comes with the gene expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first few rows of the gene expression meta-data\n",
    "data[\"datMeta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique patient identifiers in the 'patient' column of the dataFrame\n",
    "data[\"datMeta\"][\"patient\"].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc08cd8-49b9-47d7-b46e-a47c67b074c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'sample_type' column of the 'datMeta' DataFrame\n",
    "data[\"datMeta\"]['sample_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c76726",
   "metadata": {},
   "source": [
    "We are going to visualise various metadata attributes such as race, gender, sample type, cigarettes per day, and smoking status by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d80d8-48ff-4c8f-894e-f3ad6db4f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axes for a 2-column layout\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 18))\n",
    "fig.suptitle('Metadata Distributions', fontsize=20, y=0)\n",
    "\n",
    "# Plot 1: Distribution of Race\n",
    "sns.countplot(ax=axes[0, 0], x='race', data=data['datMeta'], palette='viridis')\n",
    "axes[0, 0].set_title('Distribution of Race')\n",
    "axes[0, 0].set_xlabel('Race')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Gender Distribution\n",
    "sns.countplot(ax=axes[0, 1], x='gender', data=data['datMeta'], palette='magma')\n",
    "axes[0, 1].set_title('Gender Distribution')\n",
    "axes[0, 1].set_xlabel('Gender')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Plot 3: Sample Type Distribution\n",
    "sns.countplot(ax=axes[1, 0], x='sample_type', data=data['datMeta'], palette='plasma')\n",
    "axes[1, 0].set_title('Sample Type Distribution')\n",
    "axes[1, 0].set_xlabel('Sample Type')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Distribution of Cigarettes Per Day\n",
    "sns.histplot(ax=axes[1, 1], data=data['datMeta']['cigarettes_per_day'], kde=True, color='blue')\n",
    "axes[1, 1].set_title('Distribution of Cigarettes Per Day')\n",
    "axes[1, 1].set_xlabel('Cigarettes Per Day')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 5: Smoking Status by Gender\n",
    "sns.countplot(ax=axes[2, 0], x='Smoked', hue='gender', data=data['datMeta'], palette='coolwarm')\n",
    "axes[2, 0].set_title('Smoking Status by Gender')\n",
    "axes[2, 0].set_xlabel('Smoking Status')\n",
    "axes[2, 0].set_ylabel('Count')\n",
    "axes[2, 0].legend(title='Gender')\n",
    "\n",
    "axes[2, 1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159dbc4",
   "metadata": {},
   "source": [
    "This dataset contains gene expression levels for various samples, identified by their TCGA (The Cancer Genome Atlas) codes.  \n",
    "Each row represents a different sample, while each column represents a different gene, identified by its Ensembl gene ID.  \n",
    "The values in the table are the expression levels of the genes for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd0b7a-55c6-48f1-affe-c1baae62b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a new variable for the expression data alone, just for ease of use, and then inspect it\n",
    "expression_data = data[\"datExpr\"]\n",
    "expression_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377d5da-8ee0-4d92-9e17-fe1c971f1b62",
   "metadata": {},
   "source": [
    "498 rows Ã— 22637 columns is a quite large matrix, thus we have to consider reducing the size. This could be by removing columns (genes) or sub-setting the patient samples\n",
    "\n",
    "We could:\n",
    "- Filter by Mean Expression: Select genes with high mean expression levels\n",
    "- Filter by Variance: Select genes with high variance across samples, as low variance genes might not contribute significantly to the analysis.\n",
    "- Analyse differentially expressed genes and keep ones that meet some criteria for fold-change and significance. What to compare?\n",
    "\n",
    "We are doing this to make sure our computations are  \n",
    "- computationally efficient,  \n",
    "- the network complexity is manageable,  \n",
    "- the biological signal is enhanced thus we make sure our analysis is biologically relevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6dcf97-40ba-46a2-b2cb-ffd45b0d9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few preliminary steps that might be useful for data cleaning and preprocessing\n",
    "# Ensure all columns are numeric\n",
    "expression_data = expression_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop columns that could not be converted to numeric (if any)\n",
    "expression_data = expression_data.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b221f-376e-49fb-a8bc-47d15b34ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to check the shape of the data further down the line\n",
    "expression_data.shape\n",
    "\n",
    "# note nothing has changed in the data, but we have ensured that all columns are numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1476eb2-b958-4163-9397-1eac09249dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate rows and columns\n",
    "print(f\"Number of duplicate indices: {expression_data.index.duplicated().sum()}\")  \n",
    "print(f\"Number of duplicate columns: {expression_data.columns.duplicated().sum()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa01e95-08a1-488b-b8d0-0958ca411069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of gene expression levels \n",
    "\n",
    "# Calculate the mean expression level for each gene\n",
    "gene_means = expression_data.mean(axis=0)\n",
    "\n",
    "# Plot the distribution of gene expression levels\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(gene_means, bins=50, kde=True)\n",
    "plt.xlabel('Mean Gene Expression')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Gene Expression Levels')\n",
    "\n",
    "# mean of the gene means\n",
    "threshold = gene_means.mean()\n",
    "plt.axvline(threshold, color='red', linestyle='--', label=f'Mean = {threshold:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19431a",
   "metadata": {},
   "source": [
    "The histogram shows the frequency of genes at different mean expression levels.  \n",
    "Dashed red line shows the mean of the mean gene expression.  (8.11)  \n",
    "The histogram show a bimodal distribution - does this mean two groups of genes?  \n",
    "Some genes are consistently expressed at lower levels (housekeeping genes?), while others are expressed at higher levels. \n",
    "  \n",
    "We could use this for thresholding, however it is not informative about the variability of the genes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d328df4f",
   "metadata": {},
   "source": [
    "Still, let's inspect how many genes would retain in our dataset at different expression thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9adbca65-506c-47c9-8536-b628cbfad0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out low-expressed genes from the dataset\n",
    "# As we are going to explore effects of different thresholds, we will create a function for this\n",
    "def filter_low_expression_genes(data, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Filter out low-expressed genes from the dataset.\n",
    "\n",
    "    Calculates the mean expression level for each gene and filters out\n",
    "    genes whose mean expression level is below the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): Expression data with genes as columns.\n",
    "    threshold (float): Minimum mean expression level to retain a gene.\n",
    "                       Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Filtered data with genes above the threshold.\n",
    "    \"\"\"\n",
    "    # Calculate the mean expression for each gene\n",
    "    gene_means = data.mean(axis=0)\n",
    "    # Filter out genes with mean expression below the threshold\n",
    "    mask = gene_means >= threshold\n",
    "    filtered_data = data.loc[:, mask]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec7636f-2bf4-4239-8b28-95d4cb43af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene Retention at Various Thresholds\n",
    "# Define a range of thresholds\n",
    "thresholds = np.arange(0, 15, 0.5)\n",
    "\n",
    "# List to store the number of genes retained at each threshold\n",
    "num_genes = []\n",
    "\n",
    "# Assuming df_renamed is your DataFrame with gene expression data\n",
    "for threshold in thresholds:\n",
    "    df_filtered = filter_low_expression_genes(expression_data, threshold)\n",
    "    num_genes.append(df_filtered.shape[1])\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, num_genes, marker='o')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Number of Genes Retained')\n",
    "plt.title('Number of Genes Retained at Different Thresholds')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b791d8c-fd3b-47b9-a566-a3b24216b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out genes based on their variance\n",
    "# As we are going to explore effects of variance, we will create a function for this\n",
    "def filter_high_variance_genes(data, threshold):\n",
    "    \"\"\"\n",
    "    Filter out genes with variance below the specified threshold.\n",
    "\n",
    "    Calculates the variance for each gene and filters out genes whose \n",
    "    variance is below the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): Gene expression data with genes as columns and samples as rows.\n",
    "    threshold (float): Minimum variance level to retain a gene.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Filtered data with genes having variance above the threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the variance for each gene (column)\n",
    "    gene_variances = data.var(axis=0)\n",
    "    # Create a boolean mask to filter out genes with variance below the threshold\n",
    "    mask = gene_variances >= threshold\n",
    "    # Apply the mask to filter the DataFrame\n",
    "    filtered_data = data.loc[:, mask]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac752a",
   "metadata": {},
   "source": [
    "Visualise the gene retention at different variance thresholds.  \n",
    " \n",
    "Calculate the 75th percentile of the variance distribution, and use it as a threshold.  \n",
    "It is a good balance between retaining enough data for meaningful analysis and removing low-variance noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625da1a-c271-40a9-bc37-415fad5b1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of variance thresholds\n",
    "variance_thresholds = np.arange(0, 10, 0.5)\n",
    "\n",
    "# List to store the number of genes retained at each threshold\n",
    "num_genes = []\n",
    "\n",
    "# Assuming df_renamed is your DataFrame with gene expression data\n",
    "for threshold in variance_thresholds:\n",
    "    df_filtered = filter_high_variance_genes(expression_data, threshold)\n",
    "    num_genes.append(df_filtered.shape[1])\n",
    "\n",
    "# Calculate the variance for each gene\n",
    "gene_variances = expression_data.var(axis=0)\n",
    "# Calculate the 75th percentile of the variance distribution\n",
    "variance_threshold = np.percentile(gene_variances, 75)\n",
    "print(f\"Chosen Variance Threshold: {variance_threshold }\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(variance_thresholds, num_genes, marker='o')\n",
    "plt.axvline(variance_threshold, color='red', linestyle='--', label=f'75th Percentile Threshold = {variance_threshold:.2f}')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Number of Genes Retained')\n",
    "plt.legend()\n",
    "plt.title('Number of Genes Retained at Different Variance Thresholds')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fd207",
   "metadata": {},
   "source": [
    "- Let's assign this value as threshold to focus on genes with higher variance.\n",
    "- Part of the rationale here is that if genes aren't varying much between samples they are not liklely \\\n",
    "to be differentially expressed based on any of the factors that vary between patients.\n",
    "- Note that this is not the same as differential expression analysis, as it would retain more genes including those that are not well estimated i.e. due to technical rather than biological variation.\n",
    "- This may be interesting to us because one argument we may have for making a network is that such genes would not consistently correlate with any given other set of genes (biologically) and so would not connect strongly on the network.\n",
    "- overall this is a 'softer' assumption that GXD and allows more opportunity for \"discovery\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d426e-5ed3-4b15-a669-9a1074db7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the filtered data to a new variable\n",
    "# Focusing on the top quartile (75th percentile) of genes with the highest variance\n",
    "df_filtered_variance = filter_high_variance_genes(expression_data, threshold = 1.2)\n",
    "print(f\"Filtered data shape: {df_filtered_variance.shape}\")  # Check the new shape to confirm filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94977670-a0ae-4d9c-a37a-ccfba2b29ad6",
   "metadata": {},
   "source": [
    "You may have noticed that the column header are not gene names so we're going to fix that by mapping (as we have done before in the course). You could use eUtils to do this or even bulk download the meta-data and use table merging, but we're going to use a nice package called \"mygene\" - (https://docs.mygene.info/projects/mygene-py/en/latest/)\n",
    "\n",
    "Converting Ensembl gene IDs (ENSG) to HGNC (HUGO Gene Nomenclature Committee) gene symbols is often a good practice as HGCN is an international standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bd2ff73-83bf-42b2-a305-e0537e669a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to do the gene mapping\n",
    "def rename_ensembl_to_gene_names(df, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Renames Ensembl gene IDs to gene names using mygene.\n",
    "\n",
    "    NB we chunk the requests to avoid hitting the rate limit.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with Ensembl gene IDs as columns.\n",
    "    chunk_size (int): Number of Ensembl IDs to query at a time.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with gene names as columns, excluding genes that couldn't be mapped.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Remove the `.number` suffix from ENSG IDs\n",
    "    df_copy.columns = df_copy.columns.str.split('.').str[0]\n",
    "\n",
    "    # Initialize mygene client\n",
    "    mg = mygene.MyGeneInfo()\n",
    "\n",
    "    # Split ENSG IDs into smaller chunks\n",
    "    def chunks(lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    ensg_ids = df_copy.columns.tolist()\n",
    "    gene_mappings = []\n",
    "\n",
    "    unmapped_genes = []\n",
    "\n",
    "    # send requests in chunks\n",
    "    for chunk in chunks(ensg_ids, chunk_size):\n",
    "        result = mg.querymany(chunk, scopes='ensembl.gene', fields='symbol', species='human')\n",
    "        gene_mappings.extend(result)\n",
    "\n",
    "    # Create a mapping from ENSG to gene symbol, handle missing mappings\n",
    "    ensg_to_gene = {item['query']: item.get('symbol', None) for item in gene_mappings}\n",
    "    \n",
    "    # Log the unmapped genes\n",
    "    batch_unmapped_genes = [gene for gene in ensg_ids if ensg_to_gene.get(gene) is None]\n",
    "    if batch_unmapped_genes:\n",
    "        # Add unmapped genes to the list\n",
    "        unmapped_genes.extend(batch_unmapped_genes)\n",
    "\n",
    "    # Filter the DataFrame to only include columns that have been mapped\n",
    "    df_filtered = df_copy.loc[:, df_copy.columns.isin(ensg_to_gene.keys())]\n",
    "\n",
    "    # Further filter to ensure we have the same number of columns as mapped gene names\n",
    "    df_filtered = df_filtered.loc[:, [ensg for ensg in df_filtered.columns if ensg_to_gene[ensg] is not None]]\n",
    "\n",
    "    # Assign new column names\n",
    "    df_filtered.columns = [ensg_to_gene[ensg] for ensg in df_filtered.columns]\n",
    "\n",
    "    # Handle duplicate gene names by aggregating them (e.g., by taking the mean)\n",
    "    df_final = df_filtered.T.groupby(df_filtered.columns).mean().T\n",
    "\n",
    "    return df_final, set(unmapped_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05532782-2328-42e7-a7a7-91380f248c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the ensembl gene IDs to gene names\n",
    "df_renamed,unmapped_genes = rename_ensembl_to_gene_names(df_filtered_variance)\n",
    "print(f'{len(unmapped_genes)} were not mapped to gene names.')\n",
    "\n",
    "print(f'The first unmapped gene is',unmapped_genes.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the new shape of the data after renaming\n",
    "print(\"Shape of df_filtered_variance:\", df_filtered_variance.shape)\n",
    "print(\"Shape of df_renamed:\", df_renamed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c958625-af02-4f5b-a728-cf849d32158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's inspect the first few rows of the renamed DataFrame\n",
    "df_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5008c37-1fe7-4758-9a1e-e09bdffaca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of gene expression levels \n",
    "\n",
    "# Calculate the mean expression level for each gene\n",
    "gene_means = df_renamed.mean(axis=0)\n",
    "\n",
    "# Plot the distribution of gene expression levels\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(gene_means, bins=50, kde=True)\n",
    "plt.xlabel('Mean Gene Expression')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Gene Expression Levels')\n",
    "\n",
    "# mean of the gene means\n",
    "threshold = gene_means.mean()\n",
    "plt.axvline(threshold, color='red', linestyle='--', label=f'Mean = {threshold:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76b428-8cf6-4489-aa2e-5156f65e13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and get the top 10 and bottom 10 columns based on their mean values\n",
    "\n",
    "top_10_columns = gene_means.sort_values(ascending=False).head(10)\n",
    "bottom_10_columns = gene_means.sort_values(ascending=True).head(10)\n",
    "\n",
    "# Combine top 10 and bottom 10 columns into one DataFrame\n",
    "combined = pd.concat([top_10_columns, bottom_10_columns])\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "combined.plot(kind='bar', color=['skyblue' if i < 10 else 'lightcoral' for i in range(20)])\n",
    "plt.title('Top 10 and Bottom 10 Genes Based on Mean Expression Values')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.xlabel('Gene')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf9e52-3d55-4538-a82c-9df22a556955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_columns = df_renamed.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Calculate the mean of each column for the numerical columns only\n",
    "means = numerical_columns.mean()\n",
    "\n",
    "# Get the top 8 and bottom 8 columns based on mean values\n",
    "top_8_columns = means.nlargest(8).index.tolist()\n",
    "bottom_8_columns = means.nsmallest(8).index.tolist()\n",
    "\n",
    "# Combine top 8 and bottom 8 columns for plotting\n",
    "columns_to_plot = top_8_columns + bottom_8_columns\n",
    "\n",
    "# Filter the numerical columns to only include those to plot\n",
    "filtered_numerical_columns = numerical_columns[columns_to_plot]\n",
    "\n",
    "# Calculate the number of rows needed for subplots based on the number of selected columns\n",
    "num_plots = len(filtered_numerical_columns.columns)\n",
    "num_rows = (num_plots // 4) + (num_plots % 4 > 0)  # Ensure there is an extra row if there are leftovers\n",
    "\n",
    "# Plot histograms for each selected numerical column\n",
    "fig, axes = plt.subplots(num_rows, 4, figsize=(20, 5 * num_rows))  # Adjust width and height as needed\n",
    "fig.suptitle('Distribution of Top 8 & Bottom 8 Genes Mean Expression', fontsize=16)\n",
    "\n",
    "for i, col in enumerate(filtered_numerical_columns.columns):\n",
    "    ax = axes.flatten()[i]\n",
    "    filtered_numerical_columns[col].hist(bins=15, ax=ax, color='skyblue' if i < 8 else 'lightcoral')\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "    # Adding mean and median lines\n",
    "    mean_val = filtered_numerical_columns[col].mean()\n",
    "    median_val = filtered_numerical_columns[col].median()\n",
    "    ax.axvline(mean_val, color='blue', linestyle='dashed', linewidth=1)\n",
    "    ax.axvline(median_val, color='red', linestyle='dashed', linewidth=1)\n",
    "    ax.legend({'Mean': mean_val, 'Median': median_val})\n",
    "\n",
    "# Hide any unused axes if the number of plots isn't a perfect multiple of 4\n",
    "if num_plots % 4:\n",
    "    for ax in axes.flatten()[num_plots:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae978e0",
   "metadata": {},
   "source": [
    "We now want to establish whether there are any strong relationships between the expression levels of the genes in the different samples. We can do this by calculating the correlation between their expression values across samples. We can use this correlation matrix as an adjacency matrix to build a network.\n",
    "\n",
    "- nodes: genes  \n",
    "- edges: highly correlated genes (above a given threshold)\n",
    "- edge-weights: correlation values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2aab89",
   "metadata": {},
   "source": [
    "There are a few correlation metrics one could consider:\n",
    "- [Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)  \n",
    "  - O(n^2) complexity, fast for large datasets\n",
    "- [Spearman](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
    "  -  O(n^2 log n) complexity, relatively fast but can be slower than Pearson\n",
    "- [Absolute biweight midcorrelation](https://en.wikipedia.org/wiki/Biweight_midcorrelation)\n",
    "  - Robust but slower than Pearson and Spearman, suitable for datasets with outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abeb0d10-7c8b-4399-816b-334ee563ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate absolute biweight midcorrelation \n",
    "def calc_abs_bicorr(data):\n",
    "    \"\"\"\n",
    "    Calculate the absolute biweight midcorrelation matrix for numeric data.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Input DataFrame with numeric data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the absolute biweight midcorrelation matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select only numeric data\n",
    "    data = data._get_numeric_data()\n",
    "    cols = data.columns\n",
    "    idx = cols.copy()\n",
    "    mat = data.to_numpy(dtype=float, na_value=np.nan, copy=False)\n",
    "    mat = mat.T\n",
    "\n",
    "    K = len(cols)\n",
    "    correl = np.empty((K, K), dtype=np.float32)\n",
    "\n",
    "    # Calculate biweight midcovariance\n",
    "    bicorr = astropy.stats.biweight_midcovariance(mat, modify_sample_size=True)\n",
    "\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            if i == j:\n",
    "                correl[i, j] = 1.0\n",
    "            else:\n",
    "                denominator = np.sqrt(bicorr[i, i] * bicorr[j, j])\n",
    "                if denominator != 0:\n",
    "                    correl[i, j] = bicorr[i, j] / denominator\n",
    "                else:\n",
    "                    correl[i, j] = 0  # Or handle it in another appropriate way\n",
    "\n",
    "    return pd.DataFrame(data=np.abs(correl), index=idx, columns=cols, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450b81d",
   "metadata": {},
   "source": [
    "We're going to use pre-computed correlation matrices as it takes a while to calculate. We've left the code to do this commented out below in case you would like to try it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c3f2c-9e4e-45dc-8fbf-a357e4f65174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary to store different correlation matrices\n",
    "# correlation_matrices = {}\n",
    "\n",
    "# # Pearson correlation - O(n^2) complexity, fast for large datasets\n",
    "# correlation_matrices['pearson'] = df_renamed.corr(method='pearson')\n",
    "\n",
    "# # Spearman rank correlation -  O(n^2 log n) complexity, relatively fast but can be slower than Pearson\n",
    "# correlation_matrices['spearman'] = df_renamed.corr(method='spearman')\n",
    "\n",
    "# # Biweight midcorrelation -  Robust but slower than Pearson and Spearman, suitable for datasets with outliers\n",
    "# correlation_matrices['biweight_midcorrelation'] = abs_bicorr(df_renamed)\n",
    "\n",
    "# # Print the keys of the correlation matrices to verify\n",
    "# print(\"Correlation matrices calculated:\")\n",
    "# print(correlation_matrices.keys())\n",
    "\n",
    "\n",
    "# # Save the entire dictionary of correlation matrices as a pickle file\n",
    "# with open(os.path.join(intermediate_data_dir,\"correlation_matrices.pkl\"), 'wb') as f:\n",
    "#     pickle.dump(correlation_matrices, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de27946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dictionary of correlation matrices from a pickle file\n",
    "with open(os.path.join(intermediate_data_dir,\"correlation_matrices.pkl\"), 'rb') as f:\n",
    "    correlation_matrices = pickle.load(f)\n",
    "\n",
    "# Verify the loaded data where we have each of the correlation matrices stored\n",
    "print(correlation_matrices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8da00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrices as heatmaps\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "# here we have a function to plot the correlation matrices as heatmaps\n",
    "# this is time consuming so we will not run it here but will load the pre-saved images\n",
    "def plot_correlation_matrices(correlation_matrices):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Adjust to 1 row and 3 columns\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (key, matrix) in enumerate(correlation_matrices.items()):\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(matrix, method='average')  # You can use other methods like 'single', 'complete', etc.\n",
    "        idx = leaves_list(Z)\n",
    "        \n",
    "        # Reorder matrix\n",
    "        ordered_matrix = matrix.iloc[idx, :].iloc[:, idx]\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(ordered_matrix, ax=axes[i], cmap='coolwarm', cbar=True, xticklabels=False, yticklabels=False)\n",
    "        axes[i].set_title(f'{key.capitalize()} Correlation Matrix')\n",
    "        \n",
    "        # Set square aspect ratio\n",
    "        axes[i].set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9984b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function to plot the correlation matrices\n",
    "# plot_correlation_matrices(correlation_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image of the correlation matrices\n",
    "display(Image(filename=os.path.join(intermediate_data_dir,\n",
    "                                    \"figures\",\n",
    "                                    \"correlation_matrices_figure.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32bd5b",
   "metadata": {},
   "source": [
    "We are going to define a function `create_graph_from_correlation` to make networks from correlation matrices.\n",
    "\n",
    "The function starts by creating an empty graph G. Then iterates through the columns of the correlation matrix and adds each column name as a node in the graph. This means each gene (or feature) in your dataset becomes a node in the graph.\n",
    "\n",
    "The function iterates over the upper triangle of the correlation matrix (excluding the diagonal) to avoid redundancy and self-loops. Remembering that this is an undirected graph so is symmetric.\n",
    "\n",
    "For each pair of nodes (i, j), it checks if the absolute value of the correlation coefficient between them is greater than or equal to the specified threshold.\n",
    "\n",
    "If the condition is met, an edge is added between the nodes i and j with the correlation coefficient as the weight of the edge. This signifies a strong correlation (positive or negative) between the two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7e40213-7ce1-48c8-9323-52a18a50c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the correlation matrix using a specified threshold\n",
    "def create_graph_from_correlation(correlation_matrix, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Creates a graph from a correlation matrix using a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    correlation_matrix (pd.DataFrame): DataFrame containing the correlation matrix.\n",
    "    threshold (float): Threshold for including edges based on correlation value.\n",
    "\n",
    "    Returns:\n",
    "    G (nx.Graph): Graph created from the correlation matrix.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes\n",
    "    for node in correlation_matrix.columns:\n",
    "        G.add_node(node)\n",
    "\n",
    "    # Add edges with weights above the threshold\n",
    "    for i in range(correlation_matrix.shape[0]):\n",
    "        for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "            if i != j:  # Ignore the diagonal elements\n",
    "                weight = correlation_matrix.iloc[i, j]\n",
    "                if abs(weight) >= threshold:\n",
    "                    G.add_edge(correlation_matrix.index[i], correlation_matrix.columns[j], weight=weight)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eef7ee",
   "metadata": {},
   "source": [
    "To save some time we will load the graph from a `.gml` file. A GML file is one of several commonly used  data exchange file formats for graph data. Networkx can export graphs in this and several other formats (see networkx documentation for further detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa4d8036-f692-48d5-b933-faffbd3c6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a graph from the Pearson correlation matrix with a threshold of 0.8\n",
    "# pearson_graph = create_graph_from_correlation(correlation_matrices['pearson'], threshold=0.8)\n",
    "# nx.write_gml(pearson_graph, os.path.join(data_dir,'gene_coexpression_network_pearson.gml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3aad858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the graph from the gml file\n",
    "pearson_graph = nx.read_gml(os.path.join(intermediate_data_dir,'gene_coexpression_network_pearson.gml'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3ac37",
   "metadata": {},
   "source": [
    "Now let's go through a few useful NetworkX functions and create a `print_graph_info()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecbb68c6-49f6-4f21-a91e-aa15ee100576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print basic information about the graph\n",
    "def print_graph_info(G):\n",
    "    \"\"\"\n",
    "    Print basic information about a NetworkX graph.\n",
    "\n",
    "    \n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph.\n",
    "    \"\"\"\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "    print(\"Sample nodes:\", list(G.nodes)[:10])  # Print first 10 nodes as a sample\n",
    "    print(\"Sample edges:\", list(G.edges(data=True))[:10])  # Print first 10 edges as a sample\n",
    "    \n",
    "    info_str = \"Graph type: \"\n",
    "    is_directed = G.is_directed()\n",
    "    if is_directed:\n",
    "        info_str += \"directed\"\n",
    "    else:\n",
    "        info_str += \"undirected\"\n",
    "    print(info_str)\n",
    "\n",
    "    # Check for self-loops\n",
    "    self_loops = list(nx.selfloop_edges(G))\n",
    "    if self_loops:\n",
    "        print(f\"Number of self-loops: {len(self_loops)}\")\n",
    "        print(\"Self-loops:\", self_loops)\n",
    "    else:\n",
    "        print(\"No self-loops in the graph.\")\n",
    "\n",
    "    # density of the graph\n",
    "    density = nx.density(G)\n",
    "    print(f\"Graph density: {density}\")\n",
    "\n",
    "    # Find and print the number of connected components\n",
    "    num_connected_components = nx.number_connected_components(G)\n",
    "    print(f\"Number of connected components: {num_connected_components}\")\n",
    "\n",
    "    # Calculate and print the clustering coefficient of the graph\n",
    "    clustering_coeff = nx.average_clustering(G)\n",
    "    print(f\"Average clustering coefficient: {clustering_coeff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1131c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_graph_info(pearson_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4cad886-ff46-4d4c-808f-e162ba3d8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the graph\n",
    "def visualise_graph(G, title='Gene Co-expression Network'):\n",
    "    \"\"\"\n",
    "    Visualizes the graph using Matplotlib and NetworkX.\n",
    "\n",
    "    Parameters:\n",
    "    G (nx.Graph): Graph to visualize.\n",
    "    title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G, k=0.1)  # k controls the distance between nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=50, node_color='blue', alpha=0.7)\n",
    "    nx.draw_networkx_edges(G, pos, width=0.2, alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5429f5",
   "metadata": {},
   "source": [
    "As this is time consuming to generate we will load a pre-generated image of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc77a6-a40f-453b-94a8-a06f81c43695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the graph\n",
    "# visualise_graph(pearson_graph, title='Pearson Correlation Network (Threshold = 0.8)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image of the full graph\n",
    "display(Image(filename=os.path.join(intermediate_data_dir,\n",
    "                                    \"figures\",\n",
    "                                    \"full_gene_coexpression_network.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09489b77-7091-4ac6-aead-4d76f578556b",
   "metadata": {},
   "source": [
    "We now have the base gene correlation network but we can see that there are a lot of orphans (due to the threshold filterinf and so need to clean the network up. We can use functions from NetworkX for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90d80a8e-4d7e-4d67-aa43-251d11c8e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the graph\n",
    "def clean_graph(G, degree_threshold=1, keep_largest_component=True):\n",
    "    \"\"\"\n",
    "    Cleans the graph by performing several cleaning steps:\n",
    "    - Removes unconnected nodes (isolates)\n",
    "    - Removes self-loops\n",
    "    - Removes nodes with a degree below a specified threshold\n",
    "    - Keeps only the largest connected component (optional)\n",
    "\n",
    "    Parameters:\n",
    "    G (nx.Graph): The NetworkX graph to clean.\n",
    "    degree_threshold (int): Minimum degree for nodes to keep.\n",
    "    keep_largest_component (bool): Whether to keep only the largest connected component.\n",
    "\n",
    "    Returns:\n",
    "    G (nx.Graph): Cleaned graph.\n",
    "    \"\"\"\n",
    "    G = G.copy()  # Work on a copy of the graph to avoid modifying the original graph\n",
    "\n",
    "    # Remove self-loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "\n",
    "    # Remove nodes with no edges (isolates)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "\n",
    "    # Remove nodes with degree below the threshold\n",
    "    low_degree_nodes = [node for node, degree in dict(G.degree()).items() if degree < degree_threshold]\n",
    "    G.remove_nodes_from(low_degree_nodes)\n",
    "\n",
    "    # Keep only the largest connected component\n",
    "    if keep_largest_component:\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        G = G.subgraph(largest_cc).copy()\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e3ead5d-38fc-43c7-bd82-6a9dee7db91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the graph by removing unconnected nodes\n",
    "pearson_graph_cleaned = clean_graph(pearson_graph,\n",
    "                                    degree_threshold=1,\n",
    "                                    keep_largest_component=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f2c40-de71-4323-b526-2c14575270c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the cleaned graph\n",
    "# NB this is now tractable quickly as the graph is much smaller\n",
    "visualise_graph(pearson_graph_cleaned, title='Pearson Correlation Network - Cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1cb0f-7b9c-4550-937b-7354d8d3285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can re-use the function to print the graph information\n",
    "print_graph_info(pearson_graph_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b17d1f7a-e96f-4d17-b102-3b522b5471e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the graph by keeping only the largest connected component\n",
    "pearson_graph_pruned = clean_graph(pearson_graph,\n",
    "                                    degree_threshold=1,\n",
    "                                    keep_largest_component=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8da99-c27b-43c5-b5bf-53702c3e8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_graph(pearson_graph_pruned, title='Pearson Correlation Network - Pruned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can re-use the function to print the graph information\n",
    "print_graph_info(pearson_graph_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0ba77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the degree distribution of a graph\n",
    "def plot_power_law_distribution(G):\n",
    "\n",
    "    # Compute the degree of each node\n",
    "    G_degrees = [degree for _, degree in G.degree()]\n",
    "\n",
    "    # Calculate the degree frequency distribution\n",
    "    G_degree_counts = np.bincount(G_degrees)\n",
    "    G_degree_values = np.nonzero(G_degree_counts)[0]  # degrees with at least one node\n",
    "    G_degree_probabilities = G_degree_counts[G_degree_values] / sum(G_degree_counts)\n",
    "    \n",
    "    # Plot the power law distribution\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # plot the scatter for G\n",
    "    plt.scatter(G_degree_values, G_degree_probabilities, color=\"blue\", edgecolor=\"black\", s=50, alpha=0.7)\n",
    "\n",
    "    # add a fit line for G\n",
    "    fit = stats.linregress(np.log(G_degree_values), np.log(G_degree_probabilities))\n",
    "    \n",
    "    # add the fit lines for G and G_er\n",
    "    plt.plot(G_degree_values, np.exp(fit.intercept) * G_degree_values ** fit.slope, label=f'G: {fit.slope:.2f}', color=\"blue\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Degree (log scale)\")\n",
    "    plt.ylabel(\"Probability (log scale)\")\n",
    "    plt.title(\"Degree Distribution in Log-Log Scale (Power Law)\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e829286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the degree distribution of the pruned graph\n",
    "plot_power_law_distribution(pearson_graph_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5cc14a7-9eed-4b2c-8bc4-ed79e122bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the edge weight distribution\n",
    "def visualise_edge_weight_distribution(G):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of edge weights.\n",
    "\n",
    "    Parameters:\n",
    "    edge_weights (list): List of edge weights.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    # Histogram\n",
    "    sns.histplot(edge_weights, bins=30, kde=False)\n",
    "    \n",
    "    plt.title('Distribution of Edge Weights')\n",
    "    plt.xlabel('Edge Weight')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b854688-aa87-4b76-8e40-81b5ebe5ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of edge weights\n",
    "visualise_edge_weight_distribution(pearson_graph_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00815c91-639a-4fe2-8333-291c2614e7fe",
   "metadata": {},
   "source": [
    "With sparsification we aim to reduce the number of edges in a network while preserving important structural properties.\n",
    "\n",
    "- Edge Sampling: Randomly removes a fraction of edges.\n",
    "- Thresholding: Removes edges with weights below a certain threshold.\n",
    "- Degree-based Sparsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "848d1759-bd06-4723-95f3-2afce45d2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply remove the edges below a certain edge-weight threshold\n",
    "def threshold_sparsification(graph, threshold):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by removing edges below the specified weight threshold.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    threshold (float): The weight threshold.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    sparsified_graph = nx.Graph()\n",
    "    sparsified_graph.add_nodes_from(graph_copy.nodes(data=True))\n",
    "    sparsified_graph.add_edges_from((u, v, d) for u, v, d in graph_copy.edges(data=True) if d.get('weight', 0) >= threshold)\n",
    "    return sparsified_graph\n",
    "\n",
    "# keep the specified top quantile of edges by edge-weight\n",
    "def top_percentage_sparsification(graph, top_percentage):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by keeping the top percentage of edges by weight.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    top_percentage (float): The percentage of top-weight edges to keep.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    sorted_edges = sorted(graph_copy.edges(data=True), key=lambda x: x[2].get('weight', 0), reverse=True)\n",
    "    top_edges_count = max(1, int(len(sorted_edges) * (top_percentage / 100)))\n",
    "    sparsified_graph = nx.Graph()\n",
    "    sparsified_graph.add_nodes_from(graph_copy.nodes(data=True))\n",
    "    sparsified_graph.add_edges_from(sorted_edges[:top_edges_count])\n",
    "    return sparsified_graph\n",
    "\n",
    "\n",
    "# remove nodes with degree below a certain threshold\n",
    "def remove_by_degree(graph, min_degree):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by removing nodes with degree below the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    min_degree (int): The minimum degree threshold.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    nodes_to_remove = [node for node, degree in dict(graph_copy.degree()).items() if degree < min_degree]\n",
    "    \n",
    "    graph_copy.remove_nodes_from(nodes_to_remove)\n",
    "    return graph_copy\n",
    "\n",
    "# use KNN sparsification to keep up to only the top N edges for a node\n",
    "def knn_sparsification(graph, k):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by keeping only the top-k edges with the highest weights for each node.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    k (int): The number of nearest neighbors to keep for each node.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    sparsified_graph = nx.Graph()\n",
    "    sparsified_graph.add_nodes_from(graph_copy.nodes(data=True))\n",
    "    \n",
    "    for node in graph_copy.nodes():\n",
    "        edges = sorted(graph_copy.edges(node, data=True), key=lambda x: x[2].get('weight', 0), reverse=True)\n",
    "        sparsified_graph.add_edges_from(edges[:k])\n",
    "    \n",
    "    return sparsified_graph\n",
    "\n",
    "# create a minimum spanning tree\n",
    "def spanning_tree_sparsification(graph):\n",
    "    \"\"\"\n",
    "    Sparsifies the graph by creating a minimum spanning tree.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "\n",
    "    Returns:\n",
    "    nx.Graph: The sparsified graph.\n",
    "    \"\"\"\n",
    "    graph_copy = graph.copy()\n",
    "    return nx.minimum_spanning_tree(graph_copy, weight='weight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be9a10b9-c352-41da-8582-58949b35c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to plot the density of the graph at different thresholds\n",
    "def analyse_and_plot_density(graph):\n",
    "    \"\"\"\n",
    "    Calculates and plots the density of the graph for a predefined series of thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "\n",
    "    Returns:\n",
    "    densities (list of float): Densities of the graph at each threshold.\n",
    "    \"\"\"\n",
    "    thresholds = [0.7 + i * 0.01 for i in range(31)]\n",
    "    densities = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        filtered_edges = [(u, v) for u, v, d in graph.edges(data=True) if d['weight'] > threshold]\n",
    "        temp_graph = nx.Graph()\n",
    "        temp_graph.add_edges_from(filtered_edges)\n",
    "        densities.append(nx.density(temp_graph))\n",
    "\n",
    "    # Plot the densities\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, densities, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Density vs. Threshold')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbeb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and plot the density of the graph at different thresholds\n",
    "densities = analyse_and_plot_density(pearson_graph_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3c09b-363a-49cc-99bb-2406b89b59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a dictionary to store graphs\n",
    "graphs = {}\n",
    "# Store the original graph for comparison\n",
    "graphs['original'] = pearson_graph_pruned.copy()\n",
    "\n",
    "# Apply sparsification methods to the original graph\n",
    "graphs['threshold'] = threshold_sparsification(graphs['original'], threshold=0.82)\n",
    "graphs['top_10_percent'] = top_percentage_sparsification(graphs['original'], top_percentage=10)\n",
    "graphs['degree_below_3'] = remove_by_degree(graphs['original'], min_degree=3)\n",
    "graphs['knn_5'] = knn_sparsification(graphs['original'], k=5)\n",
    "graphs['spanning_tree'] = spanning_tree_sparsification(graphs['original'])\n",
    "\n",
    "\n",
    "# Visualise the graphs after sparsification\n",
    "visualise_graph(graphs['original'], 'Original Graph')\n",
    "visualise_graph(graphs['threshold'], 'Thresholded Graph (weight > 0.82)')\n",
    "visualise_graph(graphs['top_10_percent'], 'Top 10% Edges by Weight')\n",
    "visualise_graph(graphs['degree_below_3'], 'Degree Below 3')\n",
    "visualise_graph(graphs['knn_5'], 'K-Nearest Neighbors (k=5)')\n",
    "visualise_graph(graphs['spanning_tree'], 'Minimum Spanning Tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef861253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the information of the KNN sparsified graph\n",
    "print_graph_info(graphs['knn_5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "81b493f5-1b7e-4039-aa85-8ff27b1440c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to analyse the effect of different k values on the network properties\n",
    "def analyse_knn_effect(graph, k_values):\n",
    "    \"\"\"\n",
    "    Analyses the effect of different k values on the network properties.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The original NetworkX graph.\n",
    "    k_values (list): List of k values to use for sparsification.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the analysis results.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'k': [],\n",
    "        'num_edges': [],\n",
    "        'avg_degree': [],\n",
    "        'avg_clustering': [],\n",
    "        'num_connected_components': [],\n",
    "    }\n",
    "    \n",
    "    for k in k_values:\n",
    "        sparsified_graph = knn_sparsification(graph, k)\n",
    "        num_edges = sparsified_graph.number_of_edges()\n",
    "        avg_degree = sum(dict(sparsified_graph.degree()).values()) / sparsified_graph.number_of_nodes()\n",
    "        avg_clustering = nx.average_clustering(sparsified_graph)\n",
    "        num_connected_components = nx.number_connected_components(sparsified_graph)\n",
    "        \n",
    "        results['k'].append(k)\n",
    "        results['num_edges'].append(num_edges)\n",
    "        results['avg_degree'].append(avg_degree)\n",
    "        results['avg_clustering'].append(avg_clustering)\n",
    "        results['num_connected_components'].append(num_connected_components)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# plot the analysis of the effect of different k values on network properties\n",
    "def plot_knn_analysis(df):\n",
    "    \"\"\"\n",
    "    Plots the analysis of the effect of different k values on network properties.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the analysis results.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    axes[0, 0].plot(df['k'], df['num_edges'], marker='o')\n",
    "    axes[0, 0].set_title('Number of Edges vs k')\n",
    "    axes[0, 0].set_xlabel('k')\n",
    "    axes[0, 0].set_ylabel('Number of Edges')\n",
    "    \n",
    "    axes[0, 1].plot(df['k'], df['avg_degree'], marker='o')\n",
    "    axes[0, 1].set_title('Average Degree vs k')\n",
    "    axes[0, 1].set_xlabel('k')\n",
    "    axes[0, 1].set_ylabel('Average Degree')\n",
    "    \n",
    "    axes[1, 0].plot(df['k'], df['avg_clustering'], marker='o')\n",
    "    axes[1, 0].set_title('Average Clustering Coefficient vs k')\n",
    "    axes[1, 0].set_xlabel('k')\n",
    "    axes[1, 0].set_ylabel('Average Clustering Coefficient')\n",
    "    \n",
    "    axes[1, 1].plot(df['k'], df['num_connected_components'], marker='o')\n",
    "    axes[1, 1].set_title('Number of Connected Components vs k')\n",
    "    axes[1, 1].set_xlabel('k')\n",
    "    axes[1, 1].set_ylabel('Number of Connected Components')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(1, 11))  # Different k values to analyze\n",
    "analysis_results = analyse_knn_effect(graphs['original'], k_values)\n",
    "\n",
    "# Plot the analysis results\n",
    "plot_knn_analysis(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "51b33e51-c495-46f1-b7ca-db68b0374ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to look at the top nodes based on degree\n",
    "def get_highest_degree_nodes(graph, top_n=10):\n",
    "    \"\"\"\n",
    "    Returns the nodes with the highest degree in the graph.\n",
    "\n",
    "    Parameters:\n",
    "    graph (nx.Graph): The NetworkX graph.\n",
    "    top_n (int): The number of top nodes to return.\n",
    "\n",
    "    Returns:\n",
    "    List of tuples: Each tuple contains a node and its degree.\n",
    "    \"\"\"\n",
    "    degrees = dict(graph.degree())\n",
    "    sorted_degrees = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_degrees[:top_n]\n",
    "\n",
    "# gather some information about nodes using mygene\n",
    "def fetch_gene_info(gene_list):\n",
    "    \"\"\"\n",
    "    Fetches gene information from MyGene.info.\n",
    "\n",
    "    Parameters:\n",
    "    gene_list (list): List of gene symbols or Ensembl IDs.\n",
    "\n",
    "    Returns:\n",
    "    list: List of dictionaries containing gene information.\n",
    "    \"\"\"\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    gene_info = mg.querymany(gene_list, scopes='symbol,ensembl.gene', \n",
    "                             fields='name,symbol,entrezgene,summary,disease,pathway', \n",
    "                             species='human')\n",
    "    return gene_info\n",
    "\n",
    "# combined function to report node information alongside gene metadata\n",
    "def print_gene_info_with_degree(top_genes_with_degrees, gene_info):\n",
    "    \"\"\"\n",
    "    Prints gene information including the degree.\n",
    "\n",
    "    Parameters:\n",
    "    top_genes_with_degrees (list): List of tuples containing gene symbols and their degrees.\n",
    "    gene_info (list): List of dictionaries containing gene information.\n",
    "    \"\"\"\n",
    "    for gene, degree in top_genes_with_degrees:\n",
    "        info = next((item for item in gene_info if item['query'] == gene), None)\n",
    "        if info:\n",
    "            print(f\"Gene Symbol: {info.get('symbol', 'N/A')}\")\n",
    "            print(f\"Degree: {degree}\")\n",
    "            print(f\"Gene Name: {info.get('name', 'N/A')}\")\n",
    "            print(f\"Entrez ID: {info.get('entrezgene', 'N/A')}\")\n",
    "            print(f\"Summary: {info.get('summary', 'N/A')}\")\n",
    "            if 'disease' in info:\n",
    "                diseases = ', '.join([d['term'] for d in info['disease']])\n",
    "                print(f\"Diseases: {diseases}\")\n",
    "            else:\n",
    "                print(\"Diseases: N/A\")\n",
    "            if 'pathway' in info:\n",
    "                pathways = []\n",
    "                if isinstance(info['pathway'], dict):\n",
    "                    for key in info['pathway']:\n",
    "                        pathway_data = info['pathway'][key]\n",
    "                        if isinstance(pathway_data, list):\n",
    "                            pathways.extend([p['name'] for p in pathway_data if 'name' in p])\n",
    "                        elif isinstance(pathway_data, dict) and 'name' in pathway_data:\n",
    "                            pathways.append(pathway_data['name'])\n",
    "                        elif isinstance(pathway_data, str):\n",
    "                            pathways.append(pathway_data)\n",
    "                print(f\"Pathways: {', '.join(pathways) if pathways else 'N/A'}\")\n",
    "            else:\n",
    "                print(\"Pathways: N/A\")\n",
    "            print(\"-\" * 40)\n",
    "        else:\n",
    "            print(f\"Gene not found: {gene}\")\n",
    "            print(f\"Degree: {degree}\")\n",
    "            print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f951b-f7ec-4d0c-9034-e6d9690f4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 genes with the highest degree in the pruned graph using get_highest_degree_nodes\n",
    "top_genes_with_degrees = get_highest_degree_nodes(pearson_graph_pruned, top_n=10)\n",
    "gene_symbols = [gene for gene, degree in top_genes_with_degrees]\n",
    "\n",
    "# get gene information with fetch_gene_info\n",
    "gene_info = fetch_gene_info(gene_symbols)\n",
    "\n",
    "# print gene information including degree\n",
    "print_gene_info_with_degree(top_genes_with_degrees, gene_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3333e77-a012-4aee-9b3b-1786d4c57ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pruned graph as a GML file\n",
    "#nx.write_gml(pearson_graph_pruned, \n",
    "#             os.path.join(intermediate_data_dir,'gene_coexpression_network_workshop.gml'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

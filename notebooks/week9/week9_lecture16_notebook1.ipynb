{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming for Biomedical Informatics\n",
    "#### Week 9 - Functional Analysis Using Ontologies\n",
    "\n",
    "Ontologies are commonly used to aid interpretation of molecular data, most commonly through use of functional annotations to genes and proteins using the Gene Onotlogy combined with downstream likelihood/enrichment analysis using tools such as GSEA as we have discussed. Ontologies are also used in strategies to align unstructured data with domains, for example looking for words and/or phrases that can be mapped to classes in ontologies. Examples here would include things like looking for terms associated with clinical terminolgies in patient discharge summaries.\n",
    "\n",
    "In this notebook we will perform some basic phenotype extraction from publication abstracts, attempting to find examples of HPO terms that are associated with mentions of particular diseases.\n",
    "\n",
    "To do this we will randomly select 1000 papers from PubMed that are tagged with the MeSH Major Topic \"Autism Spectrum Disorder\" retreive their titles and abstracts and then search for phenotypes and genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we're going to use a biomedical named entity recognition model called en_core_sci_sm which is a model developed by the Allen Institute for biomedical text processing\n",
    "https://allenai.github.io/scispacy/\n",
    "'''\n",
    "\n",
    "# %pip install scispacy\n",
    "# %pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for NLP\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from scispacy.linking import EntityLinker\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''when this is first run it will download some large data files to perform the UMLS linking\n",
    "approx 2GB in total. On subsequent rune the model will take about 1 minute to load'''\n",
    "\n",
    "# load the model\n",
    "nlp = spacy.load(\"en_core_sci_sm\");\n",
    "\n",
    "# add abbreviations detector\n",
    "nlp.add_pipe(\"abbreviation_detector\");\n",
    "\n",
    "# add UMLS entity-linker\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True,\"linker_name\": \"umls\",\"filter_for_definitions\": False});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at a simple example\n",
    "queryText = 'PAX6 and GLI3 is a highly conserved transcription factor that plays a critical role in eye development in all animals. Mutations in the PAX6 gene are associated with aniridia, a congenital eye malformation characterized by the absence of the iris and other eye abnormalities.'\n",
    "\n",
    "concepts = dict()\n",
    "\n",
    "try:\n",
    "    #perform nlp\n",
    "    doc = nlp(queryText)\n",
    "    for entity in doc.ents:\n",
    "        link = concept,score = entity._.kb_ents[0]\n",
    "        concepts[entity.text] = concept\n",
    "        print(entity.text, concept, score)\n",
    "except:\n",
    "    #case of no text\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the human phenotype ontology contains database_cross_reference entries that include the UMLS concept id\n",
    "# we can use this to link the HPO terms to the UMLS concepts\n",
    "\n",
    "# load the HPO data using pronto\n",
    "import pronto\n",
    "\n",
    "# load the HPO ontology\n",
    "# fetch the Human Phenotype Onology OBO file and parse it with pronto\n",
    "\n",
    "# download the HPO ontology OBO file\n",
    "import urllib.request\n",
    "\n",
    "current_hpo_url = 'http://purl.obolibrary.org/obo/hp.obo'\n",
    "\n",
    "# download the file\n",
    "urllib.request.urlretrieve(current_hpo_url,'hpo.obo');\n",
    "\n",
    "# parse the file\n",
    "hpo = pronto.Ontology('hpo.obo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look in the xrefs (sic. cross-references) of a term to find the UMLS concept id\n",
    "def hpo2concept(hpo_id):\n",
    "    term = hpo[hpo_id]\n",
    "    xrefs = [xref.id for xref in term.xrefs]\n",
    "    try:\n",
    "        umls_id = [xref for xref in xrefs if xref.startswith('UMLS')][0].split(':')[1]\n",
    "        return umls_id\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# let's test this function\n",
    "hpo2concept('HP:0001695')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're now going to brute force the conversion of all HPO terms to UMLS concepts\n",
    "hpo2umls = {term.id:hpo2concept(term.id) for term in hpo.terms()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first 10 entries\n",
    "list(hpo2umls.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see if any of the CUIs from our test sentence have been mapped to HPO terms\n",
    "for entity in concepts.keys():\n",
    "    concept = concepts[entity]\n",
    "    if concept in hpo2umls.values():\n",
    "        print(entity, concept, [hpo[k] for k,v in hpo2umls.items() if v == concept])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As a niche example (not a mainstream ontology) we will look at the ASDPTO ontology which\n",
    "has been developed as a custom ontology for autism spectrum disorder.\n",
    "\n",
    "For every term in the ASDPTO ontology, we will look for UMLS concepts.\n",
    "\n",
    "NB we are limited to the work done by ASDPTO curators in adding annotations to the terms\n",
    "'''\n",
    "\n",
    "current_asdpto_url = 'https://data.bioontology.org/ontologies/ASDPTO/submissions/1/download?apikey=4a2fbff0-ef88-432e-b1a1-dffc07e71146'\n",
    "\n",
    "# download the file\n",
    "urllib.request.urlretrieve(current_asdpto_url,'autism.obo');\n",
    "\n",
    "# parse the file\n",
    "autism  = pronto.Ontology('autism.obo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the UMLS concept for a term in ASDPTO\n",
    "def find_concept(term):\n",
    "    for annotation in term.annotations:\n",
    "        try:\n",
    "            # if the string contains a cui= then it is a UMLS concept\n",
    "            # extract the CUI\n",
    "            if 'cui=' in annotation.resource:\n",
    "                #split the string on 'cui=' and take the remainder\n",
    "                concept = annotation.resource.split('cui=')[1]\n",
    "                return(concept)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# we can now use this function to find the UMLS concept for each term in the ASDPTO ontology\n",
    "asdpto2umls = {term.name:find_concept(term) for term in autism.terms()}\n",
    "\n",
    "# remove any None entries\n",
    "asdpto2umls = {k:v for k,v in asdpto2umls.items() if v is not None}\n",
    "\n",
    "# print how many terms have been mapped to UMLS concepts\n",
    "print(f'There are ',len(asdpto2umls),' terms in the ASDPTO ontology that have been mapped to UMLS concepts')\n",
    "\n",
    "# look at the first 10 entries\n",
    "list(asdpto2umls.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now that we have all the NLP components in place to identify and map HPO terms let's now fetch the data and perform the analysis'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use our knowledge of eUtils to fetch the raw material for our analysis\n",
    "# we will use the requests library to fetch the data using the eUtils API\n",
    "# we will use the xml library to parse the data\n",
    "\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# load my API key from the file\n",
    "with open('../api_keys/ncbi.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "with open('../api_keys/ncbi_email.txt', 'r') as file:\n",
    "    email = file.read().strip()\n",
    "\n",
    "pubmed_query = '\"Autism Spectrum Disorder[Majr]\"'\n",
    "\n",
    "# Define the parameters for the eSearch request\n",
    "esearch_params = {\n",
    "    'db': 'pubmed',\n",
    "    'term': pubmed_query,\n",
    "    'api_key': api_key,\n",
    "    'email': email,\n",
    "    'usehistory': 'y'\n",
    "}\n",
    "\n",
    "# encode the parameters so they can be passed to the API\n",
    "encoded_data = urllib.parse.urlencode(esearch_params).encode('utf-8')\n",
    "\n",
    "# the base request url for eSearch\n",
    "url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "\n",
    "# make the request\n",
    "request = urllib.request.Request(url, data=encoded_data)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "# read into an XML object\n",
    "esaerch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "# print the number of results\n",
    "count = esaerch_data_XML.find('Count').text\n",
    "print(f'Total number of results: {count}')\n",
    "\n",
    "# Extract WebEnv and QueryKey\n",
    "webenv = esaerch_data_XML.find('WebEnv').text\n",
    "query_key = esaerch_data_XML.find('QueryKey').text\n",
    "\n",
    "efetch_params = {\n",
    "'db': 'pubmed',\n",
    "'query_key': query_key,\n",
    "'WebEnv': webenv,\n",
    "'retmax': '1000',\n",
    "'api_key': api_key,\n",
    "'email': email\n",
    "}\n",
    "\n",
    "# encode the parameters so they can be passed to the API\n",
    "encoded_data = urllib.parse.urlencode(efetch_params).encode('utf-8')\n",
    "\n",
    "# the base request url for eSearch\n",
    "url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "# make the request\n",
    "request = urllib.request.Request(url, data=encoded_data)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "# read into an XML object\n",
    "efetch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "# let's look at the first 10 articles\n",
    "for article in efetch_data_XML.findall('.//PubmedArticle')[:10]:\n",
    "    pmid = article.find('.//PMID').text\n",
    "    title = article.find('.//ArticleTitle').text\n",
    "    abstract = article.find('.//AbstractText').text\n",
    "    print(f'{pmid}: {title}')\n",
    "    print(f'Abstract: {abstract}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each article check whether it has an abstract and a title\n",
    "# if it does combine the title and abstract into a single string\n",
    "# if it doesn't remove it from the list\n",
    "articles = dict()\n",
    "\n",
    "for article in efetch_data_XML.findall('.//PubmedArticle'):\n",
    "    try:\n",
    "        pmid = article.find('.//PMID')\n",
    "        title = article.find('.//ArticleTitle')\n",
    "        abstract = article.find('.//AbstractText')\n",
    "        tiab = title.text + ' ' + abstract.text\n",
    "        articles[pmid.text] = tiab\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f'Number of articles with abstracts: {len(articles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets write a function based on code above to perform ner on articles and return\n",
    "def nlp_article(article):\n",
    "\n",
    "    current_article_concepts = dict()\n",
    "\n",
    "    try:\n",
    "        #perform nlp\n",
    "        doc = nlp(article)\n",
    "        for entity in doc.ents:\n",
    "            link = concept,score = entity._.kb_ents[0]\n",
    "            current_article_concepts[entity.text] = concept\n",
    "    except:\n",
    "        #case of no text\n",
    "        pass\n",
    "\n",
    "    hpo_terms = []\n",
    "\n",
    "    for entity in current_article_concepts.keys():\n",
    "        concept = current_article_concepts[entity]\n",
    "        if concept in hpo2umls.values():\n",
    "            print(entity, concept, [hpo[k] for k,v in hpo2umls.items() if v == concept])\n",
    "            current_hpo = [hpo[k] for k,v in hpo2umls.items() if v == concept]\n",
    "            hpo_terms.append(current_hpo[0].id)\n",
    "    return list(set(hpo_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can apply this function to all the articles\n",
    "articles_hpo = {pmid: nlp_article(article) for pmid, article in articles.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the results from the first 10 articles\n",
    "list(articles_hpo.items())[:10]\n",
    "\n",
    "# what percentage of articles have HPO terms\n",
    "articles_with_hpo = [k for k,v in articles_hpo.items() if v]\n",
    "print(f'Percentage of articles with HPO terms: {len(articles_with_hpo)/len(articles)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique HPO terms found in the articles\n",
    "unique_hpo_terms = list(set([term for terms in articles_hpo.values() for term in terms]))\n",
    "\n",
    "# create a dataframe to store the data\n",
    "df = pd.DataFrame(index=articles.keys(), columns=unique_hpo_terms)\n",
    "\n",
    "# fill the dataframe\n",
    "for pmid, terms in articles_hpo.items():\n",
    "    df.loc[pmid, terms] = 1\n",
    "\n",
    "# fill the NaN values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# print the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times each term appears and store this in a dataframe with columns\n",
    "# 'HPO Term Name', 'HPO Term', 'Count' and sort by count\n",
    "hpo_counts = df.sum().sort_values(ascending=False).reset_index()\n",
    "hpo_counts.columns = ['HPO Term', 'Count']\n",
    "hpo_counts['HPO Term Name'] = [hpo[term].name for term in hpo_counts['HPO Term']]\n",
    "\n",
    "# use PrettyTable to display the data\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = hpo_counts.columns\n",
    "for row in hpo_counts.itertuples(index=False):\n",
    "    table.add_row(row)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional additional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# lets speciifically look at all the DDG2P papers fom PubMed\n",
    "# read in the DDG2P data file\n",
    "ddg2p = pd.read_csv('./data/DDG2P.csv')\n",
    "ddg2p.head()\n",
    "\n",
    "# get the list of unique PMIDs from the 'pmids' column\n",
    "pmids = ddg2p['pmids'].str.split(';').explode().unique()\n",
    "\n",
    "# how many unique PMIDs are there\n",
    "print(f'Number of unique PMIDs: {len(pmids)}')\n",
    "\n",
    "# randomly select 1000 of these\n",
    "import random\n",
    "random.seed(42)\n",
    "pmids_sample = random.sample(list(pmids), 1000)\n",
    "\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# load my API key from the file\n",
    "with open('../api_keys/ncbi.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "with open('../api_keys/ncbi_email.txt', 'r') as file:\n",
    "    email = file.read().strip()\n",
    "\n",
    "# fetch the data for these PMIDs\n",
    "# Define the parameters for the eSearch request\n",
    "esearch_params = {\n",
    "    'db': 'pubmed',\n",
    "    'id': ','.join(pmids_sample),\n",
    "    'api_key': api_key,\n",
    "    'email': email,\n",
    "    'usehistory': 'y'\n",
    "}\n",
    "\n",
    "# encode the parameters so they can be passed to the API\n",
    "encoded_data = urllib.parse.urlencode(esearch_params).encode('utf-8')\n",
    "\n",
    "# the base request url for eSearch\n",
    "url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "# make the request\n",
    "request = urllib.request.Request(url, data=encoded_data)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "# read into an XML object\n",
    "efetch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "# for each article check whether it has an abstract and a title\n",
    "# if it does combine the title and abstract into a single string\n",
    "# if it doesn't remove it from the list\n",
    "articles = dict()\n",
    "\n",
    "for article in efetch_data_XML.findall('.//PubmedArticle'):\n",
    "    try:\n",
    "        pmid = article.find('.//PMID')\n",
    "        title = article.find('.//ArticleTitle')\n",
    "        abstract = article.find('.//AbstractText')\n",
    "        tiab = title.text + ' ' + abstract.text\n",
    "        articles[pmid.text] = tiab\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f'Number of articles with abstracts: {len(articles)}')\n",
    "\n",
    "# print the first 10 articles\n",
    "for pmid, tiab in list(articles.items())[:10]:\n",
    "    print(f'{pmid}: {tiab}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can apply this function to all the articles\n",
    "articles_hpo = {pmid: nlp_article(article) for pmid, article in articles.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the results from the first 10 articles\n",
    "list(articles_hpo.items())[:10]\n",
    "\n",
    "# what percentage of articles have HPO terms\n",
    "articles_with_hpo = [k for k,v in articles_hpo.items() if v]\n",
    "print(f'Percentage of articles with HPO terms: {len(articles_with_hpo)/len(articles)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique HPO terms found in the articles\n",
    "unique_hpo_terms = list(set([term for terms in articles_hpo.values() for term in terms]))\n",
    "\n",
    "# create a dataframe to store the data\n",
    "df = pd.DataFrame(index=articles.keys(), columns=unique_hpo_terms)\n",
    "\n",
    "# fill the dataframe\n",
    "for pmid, terms in articles_hpo.items():\n",
    "    df.loc[pmid, terms] = 1\n",
    "\n",
    "# fill the NaN values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# print the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times each term appears and store this in a dataframe with columns\n",
    "# 'HPO Term Name', 'HPO Term', 'Count' and sort by count\n",
    "hpo_counts = df.sum().sort_values(ascending=False).reset_index()\n",
    "hpo_counts.columns = ['HPO Term', 'Count']\n",
    "hpo_counts['HPO Term Name'] = [hpo[term].name for term in hpo_counts['HPO Term']]\n",
    "\n",
    "# use PrettyTable to display the data\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = hpo_counts.columns\n",
    "for row in hpo_counts.itertuples(index=False):\n",
    "    table.add_row(row)\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

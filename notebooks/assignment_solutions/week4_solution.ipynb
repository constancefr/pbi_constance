{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming for Biomedical Informatics\n",
    "#### Week 4 Assignment - PubMed Searching\n",
    "\n",
    "In this weekly mini assignment you will practice using eUtils to query PubMed, there are 3 examples of PubMed query in the snippets for week 4.\n",
    "\n",
    "- if you want to use the Bio:Entrez module make sure that you've installed Biopython\n",
    "- you should already have a free NCBI account so that you can get an API key, but if not please register for an NCBI account\n",
    "- you have the option of Bio:Enrtez or the ```requests``` API approach demonstrated over the last 2 weeks.\n",
    "\n",
    "We've included the basic code below based on the weekly snippets from the GitHub ```./notebooks/week4``` feel free to explore and try different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using NCBI-NLM eUtils API direclty\n",
    "\n",
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# load your API key from the file\n",
    "with open('../api_keys/ncbi.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "# load your email from the file\n",
    "with open('../api_keys/ncbi_email.txt', 'r') as file:\n",
    "    email = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_details(pubmed_id):\n",
    "\n",
    "    pubmed_id_query = f'{pubmed_id}[pmid]'\n",
    "\n",
    "    # Define the parameters for the eSearch request\n",
    "    esearch_params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': pubmed_id_query,\n",
    "        'api_key': api_key,\n",
    "        'email': email,\n",
    "        'usehistory': 'y'\n",
    "    }\n",
    "\n",
    "    # encode the parameters so they can be passed to the API\n",
    "    encoded_data = urllib.parse.urlencode(esearch_params).encode('utf-8')\n",
    "\n",
    "    # the base request url for eSearch\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "\n",
    "    # make the request\n",
    "    request = urllib.request.Request(url, data=encoded_data)\n",
    "    response = urllib.request.urlopen(request)\n",
    "\n",
    "    # read into an XML object\n",
    "    esaerch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "    # Extract WebEnv and QueryKey\n",
    "    webenv = esaerch_data_XML.find('WebEnv').text\n",
    "    query_key = esaerch_data_XML.find('QueryKey').text\n",
    "\n",
    "    efetch_params = {\n",
    "    'db': 'pubmed',\n",
    "    'query_key': query_key,\n",
    "    'WebEnv': webenv,\n",
    "    'api_key': api_key,\n",
    "    'email': email\n",
    "    }\n",
    "\n",
    "    # encode the parameters so they can be passed to the API\n",
    "    encoded_data = urllib.parse.urlencode(efetch_params).encode('utf-8')\n",
    "\n",
    "    # the base request url for eSummary\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "    # make the request\n",
    "    request = urllib.request.Request(url, data=encoded_data)\n",
    "    response = urllib.request.urlopen(request)\n",
    "\n",
    "    # read into an XML object\n",
    "    data_XML = ET.fromstring(response.read())\n",
    "\n",
    "    # extract the title\n",
    "    title = data_XML.find('.//ArticleTitle').text\n",
    "\n",
    "    # extract the authors\n",
    "    authors = data_XML.findall('.//Author')\n",
    "\n",
    "    # extract the abstract\n",
    "    abstract = data_XML.find('.//AbstractText').text\n",
    "\n",
    "    return title, authors, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Multi-Omic Graph Diagnosis (MOGDx): a data integration tool to perform classification tasks for heterogeneous diseases.\n",
      "Authors:\n",
      "\tRyan, Barry\n",
      "\tMarioni, Riccardo E\n",
      "\tSimpson, T Ian\n",
      "Abstract: Heterogeneity in human diseases presents challenges in diagnosis and treatments due to the broad range of manifestations and symptoms. With the rapid development of labelled multi-omic data, integrative machine learning methods have achieved breakthroughs in treatments by redefining these diseases at a more granular level. These approaches often have limitations in scalability, oversimplification, and handling of missing data.\n"
     ]
    }
   ],
   "source": [
    "paper_data = get_paper_details('39177104')\n",
    "\n",
    "title, authors, abstract = paper_data\n",
    "\n",
    "print(f'Title: {title}')\n",
    "print('Authors:')\n",
    "for author in authors:\n",
    "    print(f'\\t{author.find(\"LastName\").text}, {author.find(\"ForeName\").text}')\n",
    "print(f'Abstract: {abstract}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbi_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

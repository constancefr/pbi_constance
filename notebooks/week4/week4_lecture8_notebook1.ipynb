{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming for Biomedical Informatics\n",
    "#### Week 4 - Mining & Analysing the Biomedical Literature\n",
    "\n",
    "Using some of the skills we've developed working with eUtils we're now going to do some literature mining and analysis to gather some evidence about a rare genetic disease called Rett Syndrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In this script we are going to directly query eUtils using the requests library. This is because it allows us to easily specify the parameters and use the history feature of eUtils to make large queries efficiently.\n",
    "In the code below you should begin to see a pattern for how we approach the mapping process.'''\n",
    "\n",
    "# Preliminaries\n",
    "import urllib.request\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# we will use this to allow us to search the XML content retutned by the eUtils API\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# load my API key from the file\n",
    "with open('../api_keys/ncbi.txt', 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "with open('../api_keys/ncbi_email.txt', 'r') as file:\n",
    "    email = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Creating a specific search to find articles relating to Rett Syndrome\n",
    "\n",
    "'''A good place to start is to search MeSH to see if a specific term for \"Rett Syndrom\" exists. If it does we can\n",
    "use this to perform a PubMed search. Experimentally it makes sense to try a few different phrasings and methods\n",
    "to search for Rett Syndrome to see how much the results vary.\n",
    "\n",
    "https://meshb.nlm.nih.gov\n",
    "\n",
    "It would make sense to do some analysis of this; for example we could retrieve the PubMedIds for the different searches\n",
    "and comapre them to see how much overlap there is. This would give us confidence in our search strategy.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search PubMed for articles relating to Rett Syndrome using eUtils API\n",
    "# Define the parameters for the eSearch request\n",
    "\n",
    "'''as we're going to make a few different searches we can define a function to do this for us'''\n",
    "\n",
    "def pubmed_search_count(query, api_key, email):\n",
    "\n",
    "    # define the parameters for the eSearch request\n",
    "    esearch_params = {\n",
    "        'db': 'pubmed',\n",
    "        'term': query,\n",
    "        'api_key': api_key,\n",
    "        'email': email\n",
    "    }\n",
    "\n",
    "    # encode the parameters so they can be passed to the API\n",
    "    encoded_data = urllib.parse.urlencode(esearch_params).encode('utf-8')\n",
    "\n",
    "    # the base request url for eSearch\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "\n",
    "    # make the request\n",
    "    request = urllib.request.Request(url, data=encoded_data)\n",
    "    response = urllib.request.urlopen(request)\n",
    "\n",
    "    # read into an XML object\n",
    "    esaerch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "    # extract the couunt of the number of articles found\n",
    "    count = int(esaerch_data_XML.find('Count').text)\n",
    "\n",
    "    print(f\"Found {count} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a search with the MeSH term for Rett Syndrome\n",
    "'''### YOUR CODE HERE ###'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets try an allfields search for Rett Syndrome\n",
    "'''### YOUR CODE HERE ###'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets modify the function above to return the list of PubMedIds\n",
    "\n",
    "def pubmed_search_ids(query, api_key, email):\n",
    "    \n",
    "        # define the parameters for the eSearch request\n",
    "        esearch_params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': query,\n",
    "            'api_key': api_key,\n",
    "            'email': email\n",
    "        }\n",
    "\n",
    "        # encode the parameters so they can be passed to the API\n",
    "        encoded_data = urllib.parse.urlencode(esearch_params).encode('utf-8')\n",
    "\n",
    "        # the base request url for eSearch\n",
    "        url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "\n",
    "        # make the request\n",
    "        request = urllib.request.Request(url, data=encoded_data)\n",
    "        response = urllib.request.urlopen(request)\n",
    "\n",
    "        # read into an XML object\n",
    "        esaerch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "        # extract all the IDs in the response\n",
    "        ids = [id.text for id in esaerch_data_XML.findall('IdList/Id')]\n",
    "\n",
    "        return ids\n",
    "\n",
    "\n",
    "# try a search with the MeSH term for Rett Syndrome\n",
    "ids = pubmed_search_ids('\"Rett Syndrome\"[MH]', api_key, email)\n",
    "\n",
    "# print the number of ids found\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''so you can see the search by default will return 20 ids (you can modify retmax, but you'd need to adjust it based on the count return\n",
    "which in effect would call the search twice. This is why we use the history feature of eUtils to make large queries efficiently)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets modify the function above to return the list of PubMedIds\n",
    "\n",
    "def pubmed_search_ids(query, api_key, email):\n",
    "    \n",
    "        # define the parameters for the eSearch request\n",
    "        esearch_params = {\n",
    "            'db': 'pubmed',\n",
    "            'term': query,\n",
    "            'api_key': api_key,\n",
    "            'email': email,\n",
    "            'usehistory': 'y'\n",
    "        }\n",
    "\n",
    "        # encode the parameters so they can be passed to the API\n",
    "        encoded_data = urllib.parse.urlencode(esearch_params).encode('utf-8')\n",
    "\n",
    "        # the base request url for eSearch\n",
    "        url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "\n",
    "        # make the request\n",
    "        request = urllib.request.Request(url, data=encoded_data)\n",
    "        response = urllib.request.urlopen(request)\n",
    "\n",
    "        # read into an XML object\n",
    "        esearch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "        # extract the WebEnv and QueryKey\n",
    "        webenv = esearch_data_XML.find('WebEnv').text\n",
    "        query_key = esearch_data_XML.find('QueryKey').text\n",
    "\n",
    "        # define the parameters for the eSummary request\n",
    "        '''### YOUR CODE HERE ###'''\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a search with the MeSH term for Rett Syndrome\n",
    "q1_ids = pubmed_search_ids('\"Rett Syndrome\"[MH]', api_key, email)\n",
    "\n",
    "# print the number of ids found\n",
    "print(len(q1_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally a Title and Abstract restricted search\n",
    "q2_ids = '''### YOUR CODE HERE ###'''\n",
    "\n",
    "# print the number of ids found\n",
    "print(len(q2_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets use pythons set comparison to see how many ids are in common between the three searches\n",
    "'''### YOUR CODE HERE ###'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using matplotlib_venn to plot a venn diagram of the results\n",
    "\n",
    "# create the venn diagram\n",
    "venn2([set(q1_ids), set(q2_ids)], ('MeSH', 'Title and Abstract'))\n",
    "\n",
    "# show the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pick the first unique article from each search and retrieve the title and abstract\n",
    "\n",
    "# define a function to retrieve the title, abstract and authors for a given PMID\n",
    "def pubmed_fetch_tiab(pmid, api_key, email):\n",
    "        \n",
    "        # define the parameters for the eFetch request\n",
    "        efetch_params = {\n",
    "            'db': 'pubmed',\n",
    "            'id': pmid,\n",
    "            'retmode': 'xml',\n",
    "            'api_key': api_key,\n",
    "            'email': email\n",
    "        }\n",
    "\n",
    "        # encode the parameters so they can be passed to the API\n",
    "        encoded_data = urllib.parse.urlencode(efetch_params).encode('utf-8')\n",
    "\n",
    "        # the base request url for eFetch\n",
    "        url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "        # make the request\n",
    "        request = urllib.request.Request(url, data=encoded_data)\n",
    "        response = urllib.request.urlopen(request)\n",
    "\n",
    "        # read into an XML object\n",
    "        efetch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "        # remember we can print this to work out how to formulate the XML query\n",
    "        # print(ET.tostring(efetch_data_XML, encoding='utf8').decode('utf8'))\n",
    "\n",
    "        # extract the title, abstract and MeSH terms\n",
    "        '''### YOUR CODE HERE ###'''\n",
    "\n",
    "        return title, abstract, mesh_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find the list of pmids that are unique to q1_ids\n",
    "q1_unique_ids = '''### YOUR CODE HERE ###'''\n",
    "q2_unique_ids = '''### YOUR CODE HERE ###'''\n",
    "\n",
    "\n",
    "#define a function that will randomly pick a PMID from a list until one is found that has a title and abstract\n",
    "def get_tiab_from_list(ids, api_key, email):\n",
    "    '''### YOUR CODE HERE ###'''\n",
    "\n",
    "# fetch the title and abstract for the random examples\n",
    "q1_title, q1_abstract, q1_mesh_terms = get_tiab_from_list(q1_unique_ids, api_key, email)\n",
    "q2_title, q2_abstract, q2_mesh_terms = get_tiab_from_list(q2_unique_ids, api_key, email)\n",
    "\n",
    "# print the results for q1\n",
    "# print the title ALLCAPS\n",
    "'''### YOUR CODE HERE ###'''\n",
    "# wrap the abstract at 80 characters\n",
    "'''### YOUR CODE HERE ###'''\n",
    "# print the mesh terms as a tab intended list one term per line\n",
    "'''### YOUR CODE HERE ###'''\n",
    "\n",
    "# print the results for q2 as well\n",
    "'''### YOUR CODE HERE ###'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we want to try to learn something about Rett Syndrome from these papers.\n",
    "One obvious thing to do is to see look at the distrubution of MeSH terms associated with the papers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to retrieve all the MeSH terms for a list of PMIDs\n",
    "def pubmed_fetch_mesh_terms(pmids, api_key, email):\n",
    "    \n",
    "    # define the parameters for the eFetch request\n",
    "    efetch_params = {\n",
    "        'db': 'pubmed',\n",
    "        'id': ','.join(pmids),\n",
    "        'retmode': 'xml',\n",
    "        'api_key': api_key,\n",
    "        'email': email\n",
    "    }\n",
    "\n",
    "    # encode the parameters so they can be passed to the API\n",
    "    encoded_data = urllib.parse.urlencode(efetch_params).encode('utf-8')\n",
    "\n",
    "    # the base request url for eFetch\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "    # make the request\n",
    "    request = urllib.request.Request(url, data=encoded_data)\n",
    "    response = urllib.request.urlopen(request)\n",
    "\n",
    "    # read into an XML object\n",
    "    efetch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "    # extract the MeSH terms\n",
    "    mesh_terms = []\n",
    "    for article in efetch_data_XML.findall('PubmedArticle'):\n",
    "        try:\n",
    "            mesh_terms.extend([mesh.text for mesh in article.findall('MedlineCitation/MeshHeadingList/MeshHeading/DescriptorName')])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # create a dictionary of the MeSH terms and their counts\n",
    "    '''### YOUR CODE HERE ###'''\n",
    "    \n",
    "    return mesh_terms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the MeSH term data for all the \"Rett Syndrome[MH]\" papers\n",
    "pmids = q1_ids # this we derived right at the top of the notebook\n",
    "\n",
    "mesh_dict = pubmed_fetch_mesh_terms(pmids, api_key, email)\n",
    "\n",
    "# sort the dictionary by the counts\n",
    "sorted_mesh_dict = '''### YOUR CODE HERE ###'''\n",
    "\n",
    "# plot the top 10 MeSH terms\n",
    "plt.barh(list(sorted_mesh_dict.keys())[:10], list(sorted_mesh_dict.values())[:10]);\n",
    "\n",
    "'''What can we immediately learn from this data?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lets use eLink to find any OMIM records that refer to these papers.\n",
    "Remember OMIM - Online Mendelian Inheritance in Man - is a curated database of human genes and genetic disorders'''\n",
    "\n",
    "def get_pubmed_omim(pmids):\n",
    "\n",
    "    # NB you can see all the links (an astonishing number)\n",
    "    # https://eutils.ncbi.nlm.nih.gov/entrez/query/static/entrezlinks.html#pubmed\n",
    "    # we are going to use the pubmed_omim_calculated link as this finds OMIM records that cite our papers\n",
    "\n",
    "    # convert the list of pmids to a string where each pmid is preceded by 'id=' and separated by an &\n",
    "    pmid_string = '&id='.join(pmids)\n",
    "    \n",
    "    # Define the parameters for the eSearch request\n",
    "    # Notice how we don't have to specify the dbfrom and db elements if we use the linkname\n",
    "    elink_params = {\n",
    "        'linkname': 'pubmed_omim_calculated',\n",
    "        'api_key': api_key,\n",
    "        'email': email,\n",
    "    }\n",
    "    # encode the parameters so they can be passed to the API\n",
    "    encoded_data = urllib.parse.urlencode(elink_params).encode('utf-8')\n",
    "    encoded_data = encoded_data + pmid_string.encode('utf-8')\n",
    "\n",
    "    # the base request url for eSearch\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\"\n",
    "\n",
    "    # make the request\n",
    "    request = urllib.request.Request(url, data=encoded_data)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    \n",
    "    # read into an XML object\n",
    "    elink_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "    # list to store the pubmed_ids of the cited papers\n",
    "    omim_records = []\n",
    "\n",
    "    # extract the gene ids from the links\n",
    "    for link in elink_data_XML.findall('LinkSet/LinkSetDb/Link/Id'):\n",
    "        omim_records.append(link.text)\n",
    "\n",
    "    return omim_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of OMIM entries linked to the papers\n",
    "omim_records = get_pubmed_omim(pmids)\n",
    "\n",
    "# find the unique gene ids from the list\n",
    "unique_omim_records = list(set(omim_records))\n",
    "\n",
    "# print the number of unique records\n",
    "print(f'There are',len(unique_omim_records),'unique OMIM records linked to the papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use eSummary to retrieve the titles of the OMIM records\n",
    "def get_omim_titles(omim_records):\n",
    "\n",
    "    # convert the list of omim records to a string where each omim record is followed by a [MIM] in an 'OR' separated string\n",
    "    omim_string = '''### YOUR CODE HERE ###'''\n",
    "    \n",
    "    # Define the parameters for the eSearch request\n",
    "    esearch_params = {\n",
    "        'db': 'omim',\n",
    "        'term': omim_string,\n",
    "        'api_key': api_key,\n",
    "        'email': email,\n",
    "        'usehistory': 'y'\n",
    "    }\n",
    "\n",
    "    # encode the parameters so they can be passed to the API\n",
    "    encoded_data = urllib.parse.urlencode(esearch_params).encode('utf-8')\n",
    "\n",
    "    # the base request url for eSearch\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "\n",
    "    # make the request\n",
    "    request = urllib.request.Request(url, data=encoded_data)\n",
    "    response = urllib.request.urlopen(request)\n",
    "\n",
    "    # read into an XML object\n",
    "    esearch_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "    # extract the WebEnv and QueryKey\n",
    "    webenv = esearch_data_XML.find('WebEnv').text\n",
    "    query_key = esearch_data_XML.find('QueryKey').text\n",
    "\n",
    "    # define the parameters for the eSummary request\n",
    "    esummary_params = {\n",
    "        'db': 'pubmed',\n",
    "        'query_key': query_key,\n",
    "        'WebEnv': webenv,\n",
    "        'api_key': api_key,\n",
    "        'email': email\n",
    "    }\n",
    "\n",
    "    # encode the parameters so they can be passed to the API\n",
    "    encoded_data = urllib.parse.urlencode(esummary_params).encode('utf-8')\n",
    "\n",
    "    # the base request url for eSearch\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "\n",
    "    # make the request\n",
    "    request = urllib.request.Request(url, data=encoded_data)\n",
    "    response = urllib.request.urlopen(request)\n",
    "\n",
    "    # read into an XML object\n",
    "    esummary_data_XML = ET.fromstring(response.read())\n",
    "\n",
    "    # extract the titles of the OMIM records\n",
    "    omim_titles = {}\n",
    "\n",
    "    for record in esummary_data_XML.findall('DocSum'):\n",
    "        try:\n",
    "            omim_id = record.find('Id').text\n",
    "            omim_title = record.find('Item[@Name=\"Title\"]').text\n",
    "            omim_titles[omim_id] = omim_title\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return omim_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the titles of the OMIM records\n",
    "omim_titles = get_omim_titles(unique_omim_records)\n",
    "\n",
    "# convert this into a dataframe where the index is the OMIM ID and the column is the title\n",
    "'''### YOUR CODE HERE ###'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times each OMIM record is cited from the omim_records list\n",
    "omim_records_dict = {}\n",
    "\n",
    "for record in omim_records:\n",
    "    '''### YOUR CODE HERE ###'''\n",
    "\n",
    "# sort the dictionary by the counts\n",
    "sorted_omim_records_dict = '''### YOUR CODE HERE ###'''\n",
    "\n",
    "#convert the dictionary to a dataframe\n",
    "df_omim = pd.DataFrame.from_dict(sorted_omim_records_dict, orient='index', columns=['Count'])\n",
    "\n",
    "# merge the dataframe with the omim_names dataframe\n",
    "combined = '''### YOUR CODE HERE ###'''\n",
    "\n",
    "#remove the index column\n",
    "combined.reset_index(inplace=True)\n",
    "\n",
    "#rename the columns\n",
    "combined.columns = ['OMIM ID', 'Count', 'Title']\n",
    "\n",
    "# print the dataframe\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top 10 OMIM records\n",
    "'''### YOUR CODE HERE ###'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Well Done!'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbi_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
